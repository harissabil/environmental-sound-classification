{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:22:28.886807Z",
     "start_time": "2025-02-24T09:21:48.793278Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch.onnx\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx_tf\n",
    "import tensorflow as tf\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchvision.transforms import Compose"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\PycharmProjects\\environmental-audio-classification\\.venv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\haris\\PycharmProjects\\environmental-audio-classification\\.venv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.3 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:22:31.738257Z",
     "start_time": "2025-02-24T09:22:31.719237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ) if stride != 1 or in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        identity = self.downsample(identity)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "36da6529ae0db751",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:22:36.260260Z",
     "start_time": "2025-02-24T09:22:36.240352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SoundClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.con1 = nn.Conv2d(1, 16, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layers = nn.Sequential(\n",
    "            ResBlock(16, 16, 1),\n",
    "            ResBlock(16, 32, 2),\n",
    "            ResBlock(32, 32, 1),\n",
    "            ResBlock(32, 32, 1),\n",
    "            ResBlock(32, 64, 2),\n",
    "            ResBlock(64, 64, 1),\n",
    "            ResBlock(64, 64, 1),\n",
    "            ResBlock(64, 128, 2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.con1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "2c12428d0e4acffc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:22:41.030147Z",
     "start_time": "2025-02-24T09:22:39.084226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = SoundClassifier().cuda()\n",
    "classifier.load_state_dict(torch.load(\"../model/classifier.pth\"))\n",
    "classifier.eval()"
   ],
   "id": "1b4f4cbf8df03878",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoundClassifier(\n",
       "  (con1): Conv2d(1, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Identity()\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Identity()\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Identity()\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Identity()\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Identity()\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:22:44.494266Z",
     "start_time": "2025-02-24T09:22:43.513738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# export to ONNX\n",
    "dummy_input = torch.randn(1, 1, 64, 431).cuda()  # match input shape\n",
    "onnx_path = \"../model/classifier.onnx\"\n",
    "torch.onnx.export(\n",
    "    classifier,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 3: \"time_steps\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=11,\n",
    ")"
   ],
   "id": "51acca2dff5f32d2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:02.929079Z",
     "start_time": "2025-02-24T09:22:48.818240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert ONNX to TensorFlow\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "tf_rep = onnx_tf.backend.prepare(onnx_model)\n",
    "tf_path = \"../model/tf_model\"\n",
    "tf_rep.export_graph(tf_path)"
   ],
   "id": "f25dfb85b26f2a9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/tf_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/tf_model\\assets\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:23.741614Z",
     "start_time": "2025-02-24T09:23:18.623346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert TensorFlow to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "# ensure correct input shape\n",
    "converter.allow_custom_ops = True  # allow custom shapes\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # optimize model\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = \"../model/classifier.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ],
   "id": "4abf1453556b64ae",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:37.788220Z",
     "start_time": "2025-02-24T09:23:37.748609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "print(\"Expected input shape:\", input_details[0]['shape'])"
   ],
   "id": "7ea5945b8fdd9978",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape: [ 1  1 64  1]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:41.397268Z",
     "start_time": "2025-02-24T09:23:41.381759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "id": "ed28800810fc0ea6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:43.780477Z",
     "start_time": "2025-02-24T09:23:43.755910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    transform = Compose([\n",
    "        T.MelSpectrogram(sample_rate=44100, n_fft=1024, hop_length=512, n_mels=64),\n",
    "        T.AmplitudeToDB()\n",
    "    ])\n",
    "    spectrogram = transform(waveform)  # Shape: [1, 64, time_steps]\n",
    "\n",
    "    # Pad or truncate to match the expected time steps (431)\n",
    "    target_time_steps = 431\n",
    "    current_time_steps = spectrogram.shape[2]\n",
    "\n",
    "    if current_time_steps < target_time_steps:\n",
    "        # Pad with zeros\n",
    "        pad_size = target_time_steps - current_time_steps\n",
    "        spectrogram = torch.nn.functional.pad(spectrogram, (0, pad_size), \"constant\", 0)\n",
    "    elif current_time_steps > target_time_steps:\n",
    "        # Truncate\n",
    "        spectrogram = spectrogram[:, :, :target_time_steps]\n",
    "\n",
    "    # Add batch and channel dimensions\n",
    "    spectrogram = spectrogram.unsqueeze(0)  # Shape: [1, 1, 64, 431]\n",
    "    return spectrogram.numpy()"
   ],
   "id": "fe5a22f3297d0776",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:46.618348Z",
     "start_time": "2025-02-24T09:23:46.589419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run inference\n",
    "def classify_audio(audio_path, confidence_threshold=0.8):\n",
    "    input_data = preprocess_audio(audio_path)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    probabilities = tf.nn.softmax(output_data).numpy()\n",
    "    confidence = np.max(probabilities)\n",
    "    pred = np.argmax(probabilities)\n",
    "\n",
    "    if confidence < confidence_threshold:\n",
    "        print(\"Unknown audio class\")\n",
    "    else:\n",
    "        target_classes = [\n",
    "            'siren', 'car_horn', 'chainsaw', 'fireworks', 'glass_breaking',\n",
    "            'door_wood_knock', 'clock_alarm', 'crying_baby', 'thunderstorm',\n",
    "            'helicopter', 'train', 'door_wood_creaks', 'washing_machine',\n",
    "            'clapping', 'footsteps'\n",
    "        ]\n",
    "        print(f\"Audio category: {target_classes[pred]}\")"
   ],
   "id": "1b8cc56335f45d0e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T09:23:54.557866Z",
     "start_time": "2025-02-24T09:23:51.647609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example usage\n",
    "audio_path = \"../data/ESC-50-master/audio/1-101296-B-19.wav\"\n",
    "input_data = preprocess_audio(audio_path)\n",
    "interpreter.resize_tensor_input(input_details[0]['index'], [1, 1, 64, 431])\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "classify_audio(audio_path)"
   ],
   "id": "85d5ca7781e0d76a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio category: thunderstorm\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
